{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f9a6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install av\n",
    "!pip install imageio[ffmpeg]\n",
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import av  # PyAV\n",
    "import numpy as np\n",
    "import imageio\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "\n",
    "# ---------------------------\n",
    "# Utility: Video Loader Dataset\n",
    "# ---------------------------\n",
    "class VideoFolderDataset(Dataset):\n",
    "    def __init__(self, video_dir, frame_size=128, num_frames=32, transform=None):\n",
    "        self.video_paths = glob.glob(os.path.join(video_dir, '*'))\n",
    "        self.frame_size = frame_size\n",
    "        self.num_frames = num_frames\n",
    "        self.transform = transform or T.Compose([\n",
    "            T.ToTensor(),\n",
    "            T.Resize((frame_size, frame_size)),\n",
    "            T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "    \n",
    "    def _load_video(self, path):\n",
    "        container = av.open(path)\n",
    "        frames = []\n",
    "        for frame in container.decode(video=0):\n",
    "            img = frame.to_image()  # PIL Image\n",
    "            frames.append(img)\n",
    "            if len(frames) >= self.num_frames:\n",
    "                break\n",
    "        container.close()\n",
    "        while len(frames) < self.num_frames:\n",
    "            frames.append(frames[-1])\n",
    "        return frames\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.video_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        video_path = self.video_paths[idx]\n",
    "        frames = self._load_video(video_path)\n",
    "        processed_frames = torch.stack([self.transform(frame) for frame in frames], dim=0)\n",
    "        return processed_frames  # (T, C, H, W)\n",
    "\n",
    "# ---------------------------\n",
    "# Frame Encoder: ResNet50 + Projection\n",
    "# ---------------------------\n",
    "class FrameEncoder(nn.Module):\n",
    "    def __init__(self, embed_dim=512):\n",
    "        super().__init__()\n",
    "        resnet = models.resnet50(pretrained=True)\n",
    "        self.feature_extractor = nn.Sequential(*list(resnet.children())[:-2])\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Linear(2048, embed_dim)\n",
    "    \n",
    "    def forward(self, frames):\n",
    "        B, T, C, H, W = frames.shape\n",
    "        frames = frames.view(B * T, C, H, W)\n",
    "        feats = self.feature_extractor(frames)\n",
    "        pooled = self.avgpool(feats).view(B * T, 2048)\n",
    "        embeds = self.fc(pooled)\n",
    "        embeds = embeds.view(B, T, -1)\n",
    "        return embeds\n",
    "\n",
    "# ---------------------------\n",
    "# Positional Encoding\n",
    "# ---------------------------\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return x\n",
    "\n",
    "# ---------------------------\n",
    "# Temporal Transformer Encoder\n",
    "# ---------------------------\n",
    "class TemporalTransformerEncoder(nn.Module):\n",
    "    def __init__(self, embed_dim=512, nhead=8, num_layers=4, dim_feedforward=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.pos_encoder = PositionalEncoding(embed_dim)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim, nhead=nhead, dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout, batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pos_encoder(x)\n",
    "        x = self.transformer_encoder(x)\n",
    "        return x\n",
    "\n",
    "# ---------------------------\n",
    "# Multi-Video Fusion (Cross Attention)\n",
    "# ---------------------------\n",
    "class MultiVideoFusion(nn.Module):\n",
    "    def __init__(self, embed_dim=512, nhead=8):\n",
    "        super().__init__()\n",
    "        self.nhead = nhead\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "    \n",
    "    def forward(self, video_latents_list):\n",
    "        B, T, D = video_latents_list[0].shape\n",
    "        N = len(video_latents_list)\n",
    "        \n",
    "        if N == 1:\n",
    "            return video_latents_list[0]\n",
    "        \n",
    "        fused_outputs = []\n",
    "        for i in range(N):\n",
    "            query = video_latents_list[i]\n",
    "            keys, values = [], []\n",
    "            for j in range(N):\n",
    "                if j != i:\n",
    "                    keys.append(video_latents_list[j])\n",
    "                    values.append(video_latents_list[j])\n",
    "            keys = torch.cat(keys, dim=1)\n",
    "            values = torch.cat(values, dim=1)\n",
    "            \n",
    "            Q = self.q_proj(query).view(B, T, self.nhead, D // self.nhead).transpose(1, 2)\n",
    "            K = self.k_proj(keys).view(B, keys.shape[1], self.nhead, D // self.nhead).transpose(1, 2)\n",
    "            V = self.v_proj(values).view(B, values.shape[1], self.nhead, D // self.nhead).transpose(1, 2)\n",
    "            \n",
    "            scores = torch.matmul(Q, K.transpose(-2, -1)) / (D // self.nhead) ** 0.5\n",
    "            attn = torch.softmax(scores, dim=-1)\n",
    "            out = torch.matmul(attn, V)\n",
    "            \n",
    "            out = out.transpose(1, 2).contiguous().view(B, T, D)\n",
    "            out = self.out_proj(out)\n",
    "            out = self.norm(out + query)\n",
    "            fused_outputs.append(out)\n",
    "        \n",
    "        fused = torch.stack(fused_outputs, dim=0).mean(dim=0)\n",
    "        return fused\n",
    "\n",
    "# ---------------------------\n",
    "# Video Decoder\n",
    "# ---------------------------\n",
    "class VideoDecoder(nn.Module):\n",
    "    def __init__(self, embed_dim=512, frame_channels=3, frame_size=128):\n",
    "        super().__init__()\n",
    "        self.frame_size = frame_size\n",
    "        self.fc = nn.Linear(embed_dim, 2048 * 4 * 4)\n",
    "        self.deconv = nn.Sequential(\n",
    "            nn.ConvTranspose2d(2048, 1024, 4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(1024, 512, 4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(512, 256, 4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(256, 128, 4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, frame_channels, 4, stride=2, padding=1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "    \n",
    "    def forward(self, latents):\n",
    "        B, T, D = latents.shape\n",
    "        x = self.fc(latents.view(B * T, D))\n",
    "        x = x.view(B * T, 2048, 4, 4)\n",
    "        x = self.deconv(x)\n",
    "        x = x.view(B, T, 3, self.frame_size, self.frame_size)\n",
    "        return x\n",
    "\n",
    "# ---------------------------\n",
    "# Full Autoencoder\n",
    "# ---------------------------\n",
    "class VideoFusionAutoencoder(nn.Module):\n",
    "    def __init__(self, embed_dim=512, frame_size=128):\n",
    "        super().__init__()\n",
    "        self.frame_encoder = FrameEncoder(embed_dim=embed_dim)\n",
    "        self.temporal_agg = TemporalTransformerEncoder(embed_dim=embed_dim)\n",
    "        self.fusion = MultiVideoFusion(embed_dim=embed_dim)\n",
    "        self.decoder = VideoDecoder(embed_dim=embed_dim, frame_size=frame_size)\n",
    "    \n",
    "    def encode_video(self, video):\n",
    "        frame_embeds = self.frame_encoder(video)\n",
    "        latent_seq = self.temporal_agg(frame_embeds)\n",
    "        return latent_seq\n",
    "    \n",
    "    def forward(self, video_list):\n",
    "        latents = [self.encode_video(video) for video in video_list]\n",
    "        fused_latent = self.fusion(latents)\n",
    "        recon_video = self.decoder(fused_latent)\n",
    "        return recon_video, fused_latent\n",
    "\n",
    "# ---------------------------\n",
    "# Loss functions\n",
    "# ---------------------------\n",
    "def reconstruction_loss(pred, target):\n",
    "    return F.mse_loss(pred, target)\n",
    "\n",
    "def temporal_smoothness_loss(latent_seq):\n",
    "    return torch.mean((latent_seq[:, 1:] - latent_seq[:, :-1]) ** 2)\n",
    "\n",
    "# ---------------------------\n",
    "# Save video utility\n",
    "# ---------------------------\n",
    "def save_video(tensor, path, fps=10):\n",
    "    video = tensor.detach().cpu().permute(0, 2, 3, 1).numpy()\n",
    "    video = (video * 255).astype(np.uint8)\n",
    "    imageio.mimsave(path, list(video), fps=fps)\n",
    "\n",
    "# ---------------------------\n",
    "# Training Loop (with checkpoints)\n",
    "# ---------------------------\n",
    "def train(model, dataloader, optimizer, device, epochs=10, save_dir=\"./\", start_epoch=0):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    for epoch in range(start_epoch, start_epoch + epochs):\n",
    "        total_loss = 0\n",
    "        for batch_idx, videos_batch in enumerate(dataloader):\n",
    "            videos_list = [videos_batch[:, i] for i in range(videos_batch.shape[1])]\n",
    "            videos_list = [v.to(device) for v in videos_list]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            recon_video, fused_latent = model(videos_list)\n",
    "            \n",
    "            target = videos_list[0]\n",
    "            loss_recon = reconstruction_loss(recon_video, target)\n",
    "            loss_smooth = temporal_smoothness_loss(fused_latent)\n",
    "            loss = loss_recon + 0.1 * loss_smooth\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            if batch_idx % 5 == 0:\n",
    "                print(f\"Epoch {epoch+1}, Batch {batch_idx}, Loss: {loss.item():.4f}\")\n",
    "                print(f\"Time: {datetime.now(pytz.timezone('Asia/Kolkata')).strftime('%H:%M:%S')}\")\n",
    "        \n",
    "        avg_loss = total_loss / (batch_idx+1)\n",
    "        print(f\"Epoch {epoch+1} average loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        # Save fused video\n",
    "        save_path = os.path.join(save_dir, f\"bb_dunk_fused_epoch{epoch+1}.mp4\")\n",
    "        save_video(recon_video[0], save_path, fps=10)\n",
    "        print(f\"Saved fused video: {save_path}\")\n",
    "        \n",
    "        # ðŸ”¥ Save checkpoint\n",
    "        checkpoints=[40,41,42,43,44,45,46,47,48,49,50,51]\n",
    "        if (epoch+1) in checkpoints:\n",
    "            torch.save({\n",
    "                \"epoch\": epoch,\n",
    "                \"model_state_dict\": model.state_dict(),\n",
    "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            }, os.path.join(save_dir, f\"checkpoint_epoch{epoch+1}.pth\"))\n",
    "            print(f\"Checkpoint saved at epoch {epoch+1}\")\n",
    "\n",
    "# ---------------------------\n",
    "# Custom Dataset: All videos in one folder\n",
    "# ---------------------------\n",
    "class AllVideosInFolderDataset(Dataset):\n",
    "    def __init__(self, video_dir, frame_size=128, num_frames=32, transform=None):\n",
    "        self.video_paths = sorted(glob.glob(os.path.join(video_dir, '*')))\n",
    "        self.frame_size = frame_size\n",
    "        self.num_frames = num_frames\n",
    "        self.transform = transform or T.Compose([\n",
    "            T.ToTensor(),\n",
    "            T.Resize((frame_size, frame_size)),\n",
    "            T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "    \n",
    "    def _load_video(self, path):\n",
    "        container = av.open(path)\n",
    "        frames = []\n",
    "        for frame in container.decode(video=0):\n",
    "            img = frame.to_image()\n",
    "            frames.append(img)\n",
    "            if len(frames) >= self.num_frames:\n",
    "                break\n",
    "        container.close()\n",
    "        while len(frames) < self.num_frames:\n",
    "            frames.append(frames[-1])\n",
    "        return frames\n",
    "    \n",
    "    def __len__(self):\n",
    "        return 1\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        all_videos = []\n",
    "        for path in self.video_paths:\n",
    "            frames = self._load_video(path)\n",
    "            processed_frames = torch.stack([self.transform(frame) for frame in frames], dim=0)\n",
    "            all_videos.append(processed_frames)\n",
    "        all_videos = torch.stack(all_videos, dim=0)  # (N, T, C, H, W)\n",
    "        return all_videos\n",
    "\n",
    "# ---------------------------\n",
    "# Run first training session (Epochs 1 â†’ 50)\n",
    "# ---------------------------\n",
    "video_folder = \"/kaggle/input/ucf101/UCF101/UCF-101/BasketballDunk\"\n",
    "batch_size = 1\n",
    "num_frames = 32\n",
    "frame_size = 128\n",
    "epochs = 50\n",
    "learning_rate = 1e-4\n",
    "\n",
    "dataset = AllVideosInFolderDataset(video_folder, frame_size=frame_size, num_frames=num_frames)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = VideoFusionAutoencoder(embed_dim=512, frame_size=frame_size)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "train(model, dataloader, optimizer, device, epochs=epochs, save_dir=\"/kaggle/working\", start_epoch=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f410c03e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "heyff\n"
     ]
    }
   ],
   "source": [
    "# ==================================================\n",
    "# SECOND KAGGLE RUN â€” RESUME TRAINING\n",
    "# ==================================================\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2\n",
    "import numpy as np\n",
    "import imageio\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Video Dataset (same as before)\n",
    "# --------------------------------------------------\n",
    "class VideoFolderDataset(Dataset):\n",
    "    def __init__(self, folder, clip_len=16, resize=(128,128)):\n",
    "        self.folder = folder\n",
    "        self.clip_len = clip_len\n",
    "        self.resize = resize\n",
    "        self.videos = []\n",
    "        for f in os.listdir(folder):\n",
    "            if f.endswith((\".mp4\",\".avi\",\".mov\",\".mkv\")):\n",
    "                self.videos.append(os.path.join(folder,f))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.videos)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.videos[idx]\n",
    "        cap = cv2.VideoCapture(path)\n",
    "        frames=[]\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret: break\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frame = cv2.resize(frame, self.resize)\n",
    "            frame = torch.tensor(frame).permute(2,0,1).float()/255.0\n",
    "            frames.append(frame)\n",
    "        cap.release()\n",
    "        frames = torch.stack(frames)\n",
    "\n",
    "        if frames.shape[0] >= self.clip_len:\n",
    "            start = np.random.randint(0, frames.shape[0]-self.clip_len+1)\n",
    "            clip = frames[start:start+self.clip_len]\n",
    "        else:\n",
    "            pad = self.clip_len - frames.shape[0]\n",
    "            clip = torch.cat([frames, frames[-1:].repeat(pad,1,1,1)],0)\n",
    "\n",
    "        return clip\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Frame Encoder (ResNet backbone)\n",
    "# --------------------------------------------------\n",
    "class FrameEncoder(nn.Module):\n",
    "    def __init__(self, embed_dim=512):\n",
    "        super().__init__()\n",
    "        base = models.resnet18(pretrained=True)\n",
    "        self.feature_extractor = nn.Sequential(*list(base.children())[:-1])\n",
    "        self.fc = nn.Linear(base.fc.in_features, embed_dim)\n",
    "\n",
    "    def forward(self,x):  # x: [B,3,H,W]\n",
    "        feat = self.feature_extractor(x).view(x.size(0),-1)\n",
    "        return self.fc(feat)\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Simple Transformer\n",
    "# --------------------------------------------------\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, embed_dim=512, num_heads=8, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(embed_dim,num_heads,dropout=dropout)\n",
    "        self.ln1 = nn.LayerNorm(embed_dim)\n",
    "        self.ln2 = nn.LayerNorm(embed_dim)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(embed_dim,2048), nn.ReLU(), nn.Linear(2048,embed_dim)\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        attn_out,_ = self.attn(x,x,x)\n",
    "        x = self.ln1(x+attn_out)\n",
    "        ff_out = self.ff(x)\n",
    "        return self.ln2(x+ff_out)\n",
    "\n",
    "class TemporalTransformerEncoder(nn.Module):\n",
    "    def __init__(self, embed_dim=512, num_layers=2, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([TransformerEncoderLayer(embed_dim,num_heads) for _ in range(num_layers)])\n",
    "    def forward(self,x):\n",
    "        for l in self.layers: x = l(x)\n",
    "        return x\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Video Decoder\n",
    "# --------------------------------------------------\n",
    "class VideoDecoder(nn.Module):\n",
    "    def __init__(self, embed_dim=512, frame_size=128, clip_len=16):\n",
    "        super().__init__()\n",
    "        self.clip_len = clip_len\n",
    "        self.fc = nn.Linear(embed_dim,512*4*4)\n",
    "        self.deconv = nn.Sequential(\n",
    "            nn.ConvTranspose2d(512,256,4,2,1), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(256,128,4,2,1), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128,64,4,2,1), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64,32,4,2,1), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32,3,4,2,1), nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self,z): # z: [B,E]\n",
    "        x = self.fc(z).view(z.size(0),512,4,4)\n",
    "        frame = self.deconv(x)\n",
    "        return frame\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Fusion + Autoencoder\n",
    "# --------------------------------------------------\n",
    "class FusionModule(nn.Module):\n",
    "    def __init__(self, embed_dim=512):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(embed_dim*2, embed_dim)\n",
    "    def forward(self,a,b): return self.fc(torch.cat([a,b],dim=-1))\n",
    "\n",
    "class VideoFusionAutoencoder(nn.Module):\n",
    "    def __init__(self, embed_dim=512, frame_size=128, clip_len=16):\n",
    "        super().__init__()\n",
    "        self.frame_encoder = FrameEncoder(embed_dim)\n",
    "        self.temporal = TemporalTransformerEncoder(embed_dim)\n",
    "        self.fusion = FusionModule(embed_dim)\n",
    "        self.decoder = VideoDecoder(embed_dim,frame_size,clip_len)\n",
    "\n",
    "    def forward(self, videos_list):\n",
    "        B = videos_list[0].size(0)\n",
    "        encs=[]\n",
    "        for v in videos_list:\n",
    "            T = v.size(1)\n",
    "            frames = v.view(-1,3,v.size(-2),v.size(-1))\n",
    "            latent = self.frame_encoder(frames).view(B,T,-1).transpose(0,1)\n",
    "            latent = self.temporal(latent)\n",
    "            pooled = latent.mean(0)\n",
    "            encs.append(pooled)\n",
    "        fused = encs[0]\n",
    "        for e in encs[1:]:\n",
    "            fused = self.fusion(fused,e)\n",
    "        dec = self.decoder(fused).unsqueeze(1).repeat(1,videos_list[0].size(1),1,1,1)\n",
    "        return dec,fused\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Losses + Video Saver\n",
    "# --------------------------------------------------\n",
    "def reconstruction_loss(pred,target): return F.mse_loss(pred,target)\n",
    "def temporal_smoothness_loss(z): return ((z[1:]-z[:-1])**2).mean()\n",
    "def save_video(frames, path, fps=10):\n",
    "    frames = (frames.detach().cpu().permute(0,2,3,1).numpy()*255).astype(np.uint8)\n",
    "    imageio.mimwrite(path, frames, fps=fps, codec=\"libx264\")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Training Loop (with resume support)\n",
    "# --------------------------------------------------\n",
    "def train(model,dataloader,optimizer,device,epochs=10,save_dir=\"./\",start_epoch=0):\n",
    "    model.to(device); model.train()\n",
    "    for epoch in range(start_epoch, start_epoch+epochs):\n",
    "        total_loss=0\n",
    "        for b, videos_batch in enumerate(dataloader):\n",
    "            videos_list=[videos_batch[:,i].to(device) for i in range(videos_batch.size(1))]\n",
    "            optimizer.zero_grad()\n",
    "            recon, z = model(videos_list)\n",
    "            target=videos_list[0]\n",
    "            loss = reconstruction_loss(recon,target)+0.1*temporal_smoothness_loss(z)\n",
    "            loss.backward(); optimizer.step()\n",
    "            total_loss+=loss.item()\n",
    "            if b%5==0: print(f\"Epoch {epoch+1}, Batch {b}, Loss {loss.item():.4f}\")\n",
    "        print(f\"Epoch {epoch+1} avg loss: {total_loss/(b+1):.4f}\")\n",
    "        save_video(recon[0], os.path.join(save_dir,f\"fused_epoch{epoch+1}.mp4\"))\n",
    "        checkpoints2=[90,92,94,96,98,100,102]\n",
    "        if (epoch+1) in checkpoints2:\n",
    "            torch.save({\n",
    "                \"epoch\":epoch,\n",
    "                \"model_state_dict\":model.state_dict(),\n",
    "                \"optimizer_state_dict\":optimizer.state_dict(),\n",
    "            }, os.path.join(save_dir,f\"checkpoint_epoch{epoch+1}.pth\"))\n",
    "            print(f\"Checkpoint saved at epoch {epoch+1}\")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Load Data\n",
    "# --------------------------------------------------\n",
    "dataset_path= \"/kaggle/input/ucf101/UCF101/UCF-101/BasketballDunk\"\n",
    "dataset=VideoFolderDataset(dataset_path,clip_len=16,resize=(128,128))\n",
    "dataloader=DataLoader(dataset,batch_size=2,shuffle=True)\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Resume from Last Checkpoint\n",
    "# --------------------------------------------------\n",
    "device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model=VideoFusionAutoencoder(embed_dim=512,frame_size=128,clip_len=16).to(device)\n",
    "optimizer=torch.optim.Adam(model.parameters(),lr=1e-4)\n",
    "\n",
    "# ðŸ”¥ Load the checkpoint you uploaded from run 1\n",
    "checkpoint=torch.load(\"/kaggle/working/checkpoint_epoch50.pth\")\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "start_epoch=checkpoint[\"epoch\"]+1\n",
    "\n",
    "print(f\"Resuming training from epoch {start_epoch}\")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Continue Training (e.g., 50 more epochs)\n",
    "# --------------------------------------------------\n",
    "train(model,dataloader,optimizer,device,\n",
    "      epochs=50, #+50 from the previous block\n",
    "      save_dir=\"/kaggle/working\",\n",
    "      start_epoch=start_epoch)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
